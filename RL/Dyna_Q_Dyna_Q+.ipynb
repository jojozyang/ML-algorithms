{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd432bf-5125-4929-b316-75b413671fbf",
   "metadata": {},
   "source": [
    "# Dyna-Q and Dyna-Q+\n",
    "\n",
    "In this notebook, a Shortcut Maze environment is solved by Dyna-Q and a changing maze environment by Dyna-Q+.\n",
    "\n",
    "The code is adapted from 'Reinforcement Learning Specialization' course (University of Alberta) to accomplish the homework assignment. \n",
    "\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Import Packages <img align=\"Right\" src=\"./images/shortcut_env_after.png\" width = 80%>](#1)\n",
    "- [ 2 - Algorithm Overview](#2)\n",
    "- [ 3 - Main Functions](#3)\n",
    "- [ 4 - Helper Functions](#4)\n",
    "- [ 5 - Train the Agent](#5)\n",
    "- [ 6 - Visualize the results](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb68fa-a7d6-4863-9a38-db78b0cd3e4a",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84c814-809c-40b7-817b-ee37a92e7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jdc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl_glue import RLGlue # the library for reinforcement learning experiments\n",
    "from agent import BaseAgent\n",
    "from maze_env import ShortcutMazeEnvironment\n",
    "\n",
    "import os\n",
    "os.chdir('./RL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54420f1-e511-40c0-b16c-ba3c07870917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'figure.figsize': [8,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f64687-942a-42f0-b58b-c8d5167fbf12",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Algorithm Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a8392-7915-40be-a98e-3d2dec657600",
   "metadata": {},
   "source": [
    "<img align=\"Center\" src=\"./images/DynaQ.png\" width = 50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653a68e-1d3a-4523-97c7-e3ea83150804",
   "metadata": {},
   "source": [
    "The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven't been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.\n",
    "\n",
    "In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\\tau(s,a)$ time steps, then planning updates are done as if that transition produced a reward of $r + \\kappa \\sqrt{ \\tau(s,a)}$, for some small $\\kappa$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4efa5-b583-494a-987e-6cc45183d2eb",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a80fa-5744-4415-9dfc-ad7fa82d6d1a",
   "metadata": {},
   "source": [
    "### 3.1 - Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6b49a-ec37-4ba4-813d-a4af41da8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQAgent(BaseAgent):\n",
    "\n",
    "    def agent_init(self, agent_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Args:\n",
    "            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "            {\n",
    "                num_states (int): The number of states,\n",
    "                num_actions (int): The number of actions,\n",
    "                epsilon (float): The parameter for epsilon-greedy exploration,\n",
    "                step_size (float): The step-size,\n",
    "                discount (float): The discount factor,\n",
    "                planning_steps (int): The number of planning steps per environmental interaction\n",
    "\n",
    "                random_seed (int): the seed for the RNG used in epsilon-greedy\n",
    "                planning_random_seed (int): the seed for the RNG used in the planner\n",
    "            }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.num_states = agent_info[\"num_states\"]\n",
    "            self.num_actions = agent_info[\"num_actions\"]\n",
    "        except:\n",
    "            print(\"You need to pass both 'num_states' and 'num_actions' \\\n",
    "                   in agent_info to initialize the action-value table\")\n",
    "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n",
    "        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n",
    "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
    "        self.actions = list(range(self.num_actions))\n",
    "        self.past_action = -1\n",
    "        self.past_state = -1\n",
    "        self.model = {} # model is a dictionary of dictionaries, which maps states to actions to \n",
    "                        # (reward, next_state) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a00b-0c9f-4ecb-9382-76ec7b0a4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DynaQAgent\n",
    "def argmax(self, q_values):\n",
    "    \"\"\"argmax with random tie-breaking\n",
    "    Args:\n",
    "        q_values (Numpy array): the array of action values\n",
    "    Returns:\n",
    "        action (int): an action with the highest value\n",
    "    \"\"\"\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return self.rand_generator.choice(ties)\n",
    "\n",
    "def choose_action_egreedy(self, state):\n",
    "    \"\"\"returns an action using an epsilon-greedy policy w.r.t. the current action-value function.\n",
    "\n",
    "    Important: assume you have a random number generator 'rand_generator' as a part of the class\n",
    "                which you can use as self.rand_generator.choice() or self.rand_generator.rand()\n",
    "\n",
    "    Args:\n",
    "        state (List): coordinates of the agent (two elements)\n",
    "    Returns:\n",
    "        The action taken w.r.t. the aforementioned epsilon-greedy policy\n",
    "    \"\"\"\n",
    "\n",
    "    if self.rand_generator.rand() < self.epsilon:\n",
    "        action = self.rand_generator.choice(self.actions)\n",
    "    else:\n",
    "        values = self.q_values[state]\n",
    "        action = self.argmax(values)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8c0c5-bec5-41cc-9fb4-e5977547648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_episodes = exp_parameters['num_episodes']\n",
    "    planning_steps_all = agent_parameters['planning_steps']\n",
    "\n",
    "    env_info = env_parameters                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  # We pass the agent the information it needs. \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"], \n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) # for collecting metrics \n",
    "    log_data = {'planning_steps_all' : planning_steps_all}                     # that shall be plotted later\n",
    "\n",
    "    for idx, planning_steps in enumerate(planning_steps_all):\n",
    "\n",
    "        print('Planning steps : ', planning_steps)\n",
    "        os.system('sleep 0.5')                    # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"planning_steps\"] = planning_steps  \n",
    "\n",
    "        for i in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['random_seed'] = i\n",
    "            agent_info['planning_random_seed'] = i\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            for j in range(num_episodes):\n",
    "\n",
    "                rl_glue.rl_start()                # We start an episode. Here we aren't using rl_glue.rl_episode()\n",
    "                                                  # like the other assessments because we'll be requiring some \n",
    "                is_terminal = False               # data from within the episodes in some of the experiments here \n",
    "                num_steps = 0\n",
    "                while not is_terminal:\n",
    "                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step \n",
    "                    num_steps += 1                                      # and return the reward and action taken.\n",
    "\n",
    "                all_averages[idx][i][j] = num_steps\n",
    "\n",
    "    log_data['all_averages'] = all_averages\n",
    "    \n",
    "    return log_data\n",
    "    \n",
    "\n",
    "def plot_steps_per_episode(data):\n",
    "    all_averages = data['all_averages']\n",
    "    planning_steps_all = data['planning_steps_all']\n",
    "\n",
    "    for i, planning_steps in enumerate(planning_steps_all):\n",
    "        plt.plot(np.mean(all_averages[i], axis=0), label='Planning steps = '+str(planning_steps))\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n",
    "    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f2107-3e67-4ff1-9720-4fa69d884674",
   "metadata": {},
   "source": [
    "### 3.1 - Dyna-Q+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743776c1-f27a-4944-ad05-d0f734c82c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "\n",
    "class DynaQPlusAgent(BaseAgent):\n",
    "    \n",
    "    def agent_init(self, agent_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Args:\n",
    "            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "            {\n",
    "                num_states (int): The number of states,\n",
    "                num_actions (int): The number of actions,\n",
    "                epsilon (float): The parameter for epsilon-greedy exploration,\n",
    "                step_size (float): The step-size,\n",
    "                discount (float): The discount factor,\n",
    "                planning_steps (int): The number of planning steps per environmental interaction\n",
    "                kappa (float): The scaling factor for the reward bonus\n",
    "\n",
    "                random_seed (int): the seed for the RNG used in epsilon-greedy\n",
    "                planning_random_seed (int): the seed for the RNG used in the planner\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we get the relevant information from agent_info \n",
    "        # Note: we use np.random.RandomState(seed) to set the two different RNGs\n",
    "        # for the planner and the rest of the code\n",
    "        try:\n",
    "            self.num_states = agent_info[\"num_states\"]\n",
    "            self.num_actions = agent_info[\"num_actions\"]\n",
    "        except:\n",
    "            print(\"You need to pass both 'num_states' and 'num_actions' \\\n",
    "                   in agent_info to initialize the action-value table\")\n",
    "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
    "        self.kappa = agent_info.get(\"kappa\", 0.001)\n",
    "\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n",
    "        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n",
    "\n",
    "        # Next, we initialize the attributes required by the agent, e.g., q_values, model, tau, etc.\n",
    "        # The visitation-counts can be stored as a table as well, like the action values \n",
    "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
    "        self.tau = np.zeros((self.num_states, self.num_actions))\n",
    "        self.actions = list(range(self.num_actions))\n",
    "        self.past_action = -1\n",
    "        self.past_state = -1\n",
    "        self.model = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3070b3b-34e3-4894-8b4d-41a9959fe580",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99842a8c-998a-4862-b792-456f3e0afc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c2d1a9-0cc9-45f9-9524-fa750db9771d",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f7fc6-0aa1-4ccf-9bcb-3ffcf603d531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1dee918-b51e-4b5f-a9dc-15a3e0648952",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Visualize the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f6b79-56ec-4c39-9cb6-533337428c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
